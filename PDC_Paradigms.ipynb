{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkKnjL8VPvaVwW+R8bCGzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmer-talal/Colab-Files/blob/main/PDC_Paradigms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ahmer Talal   SP23-BCS-041**"
      ],
      "metadata": {
        "id": "1JPc93LiyMnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulating a Fog computing scenario where multiple edge devices collect sensor data (temperature, motion, etc.) and process it in parallel threads before forwarding aggregated results to a simulated cloud server. Use threading or multiprocessing to represent parallelism at the fog layer:"
      ],
      "metadata": {
        "id": "oqB3SyWhyGIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a6p48RoCxvdo",
        "outputId": "c1b26dda-cf5d-40b0-ca92-28af079897ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fog] Device 1 collected temperature: 27.38, processed: 30.118000000000002\n",
            "[Fog] Device 2 collected motion: 1, processed: 1.1\n",
            "[Fog] Device 3 collected humidity: 42.99, processed: 47.28900000000001\n",
            "[Fog] Device 4 collected temperature: 31.65, processed: 34.815\n",
            "\n",
            "[Fog] Aggregated Result: 28.3305\n",
            "\n",
            "--- Sending Data to Cloud ---\n",
            "Aggregated Fog Layer Data: 28.3305\n",
            "Cloud Processing Complete!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import random\n",
        "import time\n",
        "\n",
        "# ---------------------------\n",
        "# Simulated Edge Device Class\n",
        "# ---------------------------\n",
        "class EdgeDevice(threading.Thread):\n",
        "    def __init__(self, device_id, sensor_type, results):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.device_id = device_id\n",
        "        self.sensor_type = sensor_type\n",
        "        self.results = results\n",
        "\n",
        "    def collect_sensor_data(self):\n",
        "        if self.sensor_type == \"temperature\":\n",
        "            return round(random.uniform(20, 35), 2)\n",
        "        elif self.sensor_type == \"motion\":\n",
        "            return random.choice([0, 1])  # 0=no motion, 1=motion detected\n",
        "        elif self.sensor_type == \"humidity\":\n",
        "            return round(random.uniform(30, 60), 2)\n",
        "\n",
        "    def process_data(self, value):\n",
        "        # simple processing: normalization\n",
        "        return value * 1.1\n",
        "\n",
        "    def run(self):\n",
        "        raw_value = self.collect_sensor_data()\n",
        "        processed = self.process_data(raw_value)\n",
        "\n",
        "        print(f\"[Fog] Device {self.device_id} collected {self.sensor_type}: {raw_value}, processed: {processed}\")\n",
        "        self.results.append(processed)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Cloud Server Function\n",
        "# ---------------------------\n",
        "def cloud_server(aggregated_data):\n",
        "    print(\"\\n--- Sending Data to Cloud ---\")\n",
        "    print(f\"Aggregated Fog Layer Data: {aggregated_data}\")\n",
        "    print(\"Cloud Processing Complete!\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# MAIN FOG COMPUTING SIMULATION\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    results = []\n",
        "\n",
        "    devices = [\n",
        "        EdgeDevice(1, \"temperature\", results),\n",
        "        EdgeDevice(2, \"motion\", results),\n",
        "        EdgeDevice(3, \"humidity\", results),\n",
        "        EdgeDevice(4, \"temperature\", results)\n",
        "    ]\n",
        "\n",
        "    # Run all devices in parallel\n",
        "    for d in devices:\n",
        "        d.start()\n",
        "\n",
        "    for d in devices:\n",
        "        d.join()  # wait for all threads to finish\n",
        "\n",
        "    # Fog layer aggregated output\n",
        "    fog_output = sum(results) / len(results)\n",
        "    print(f\"\\n[Fog] Aggregated Result: {fog_output}\")\n",
        "\n",
        "    # Forward to cloud\n",
        "    cloud_server(fog_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hadoop MapReduce vs Apache Spark**"
      ],
      "metadata": {
        "id": "NZv8oS4Kzbvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MapReduce Simulation:"
      ],
      "metadata": {
        "id": "s8D3bXLgzfPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark Implementation (PySpark)"
      ],
      "metadata": {
        "id": "puu-TlLZ0SmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "import time\n",
        "\n",
        "def mapper(line):\n",
        "    temp = int(line.split(\",\")[0])\n",
        "    return temp\n",
        "\n",
        "def reducer(values):\n",
        "    return sum(values) / len(values)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with open(\"weather_data.txt\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with Pool() as p:\n",
        "    mapped = p.map(mapper, lines)\n",
        "\n",
        "result = reducer(mapped)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"MapReduce Average Temperature:\", result)\n",
        "print(\"Execution Time (MapReduce):\", end - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5Km4-eWx1TPG",
        "outputId": "a82521c6-00c8-40ff-8916-f78066041a9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MapReduce Average Temperature: 30.395833333333332\n",
            "Execution Time (MapReduce): 0.022944211959838867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1MLV_NEo0W2V",
        "outputId": "c0d70456-0417-44df-cb3f-d98e40876893"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import time\n",
        "\n",
        "sc = SparkContext(\"local\", \"WeatherApp\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "rdd = sc.textFile(\"weather_data.txt\")\n",
        "temps = rdd.map(lambda line: int(line.split(\",\")[0]))\n",
        "avg = temps.mean()\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"Spark Average Temperature:\", avg)\n",
        "print(\"Execution Time (Spark):\", end - start)\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PyivM_ve1YqO",
        "outputId": "4627d07d-e5d7-4afb-bb4e-978090a9b187"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Average Temperature: 30.395833333333336\n",
            "Execution Time (Spark): 1.6353590488433838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison Result:**"
      ],
      "metadata": {
        "id": "NXRvSE0-2OPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment of 50-row weather dataset, **Hadoop MapReduce**(Python multiprocessing) computed an average **temperature of 30.39°C** with an execution **time of only 0.02 seconds**, showing efficient performance on small data. Apache Spark produced the same average **temperature (30.39°C)** but took **1.63 seconds** due to its ***JVM and RDD initialization overhead.*** This shows that **MapReduce is faster for small datasets**, while Spark is designed to outperform MapReduce on large-scale, in-memory processing tasks."
      ],
      "metadata": {
        "id": "GfwMTXsW2TxL"
      }
    }
  ]
}